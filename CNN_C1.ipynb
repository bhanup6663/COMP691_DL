{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset,DataLoader, Subset, random_split\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = np.random.choice(range(10), 2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Additional conv layer\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0\n",
    "best_model_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(val_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss: 1.3853, Train Acc: 40.00%, Val Loss: 2.1202, Val Acc: 70.00%\n",
      "Starting epoch 2\n",
      "Train Loss: 0.5847, Train Acc: 77.50%, Val Loss: 0.3866, Val Acc: 80.00%\n",
      "Starting epoch 3\n",
      "Train Loss: 0.7506, Train Acc: 70.00%, Val Loss: 0.7802, Val Acc: 70.00%\n",
      "Starting epoch 4\n",
      "Train Loss: 1.3162, Train Acc: 67.50%, Val Loss: 0.6277, Val Acc: 70.00%\n",
      "Starting epoch 5\n",
      "Train Loss: 1.0114, Train Acc: 65.00%, Val Loss: 0.4434, Val Acc: 90.00%\n",
      "Starting epoch 6\n",
      "Train Loss: 0.9994, Train Acc: 65.00%, Val Loss: 1.4553, Val Acc: 40.00%\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Starting epoch 7\n",
      "Train Loss: 0.5768, Train Acc: 77.50%, Val Loss: 1.1201, Val Acc: 50.00%\n",
      "Starting epoch 8\n",
      "Train Loss: 0.9523, Train Acc: 65.00%, Val Loss: 0.9074, Val Acc: 50.00%\n",
      "Starting epoch 9\n",
      "Train Loss: 0.7643, Train Acc: 77.50%, Val Loss: 0.8773, Val Acc: 50.00%\n",
      "Starting epoch 10\n",
      "Train Loss: 0.6869, Train Acc: 72.50%, Val Loss: 0.9001, Val Acc: 50.00%\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Starting epoch 11\n",
      "Train Loss: 0.8139, Train Acc: 67.50%, Val Loss: 0.9154, Val Acc: 50.00%\n",
      "Starting epoch 12\n",
      "Train Loss: 0.5065, Train Acc: 80.00%, Val Loss: 0.9274, Val Acc: 50.00%\n",
      "Starting epoch 13\n",
      "Train Loss: 0.5476, Train Acc: 77.50%, Val Loss: 0.8610, Val Acc: 50.00%\n",
      "Starting epoch 14\n",
      "Train Loss: 0.8012, Train Acc: 67.50%, Val Loss: 0.8275, Val Acc: 50.00%\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Starting epoch 15\n",
      "Train Loss: 0.5908, Train Acc: 72.50%, Val Loss: 0.8228, Val Acc: 50.00%\n",
      "Starting epoch 16\n",
      "Train Loss: 0.7186, Train Acc: 70.00%, Val Loss: 0.8709, Val Acc: 50.00%\n",
      "Starting epoch 17\n",
      "Train Loss: 0.8323, Train Acc: 60.00%, Val Loss: 0.9060, Val Acc: 50.00%\n",
      "Starting epoch 18\n",
      "Train Loss: 0.8540, Train Acc: 67.50%, Val Loss: 0.8722, Val Acc: 50.00%\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Starting epoch 19\n",
      "Train Loss: 0.7045, Train Acc: 75.00%, Val Loss: 0.8692, Val Acc: 50.00%\n",
      "Starting epoch 20\n",
      "Train Loss: 0.6663, Train Acc: 77.50%, Val Loss: 0.8612, Val Acc: 50.00%\n",
      "Starting epoch 21\n",
      "Train Loss: 0.5715, Train Acc: 82.50%, Val Loss: 0.8927, Val Acc: 50.00%\n",
      "Starting epoch 22\n",
      "Train Loss: 0.7059, Train Acc: 67.50%, Val Loss: 0.8848, Val Acc: 50.00%\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Starting epoch 23\n",
      "Train Loss: 0.8717, Train Acc: 60.00%, Val Loss: 0.8852, Val Acc: 50.00%\n",
      "Starting epoch 24\n",
      "Train Loss: 0.6379, Train Acc: 75.00%, Val Loss: 0.8976, Val Acc: 50.00%\n",
      "Starting epoch 25\n",
      "Train Loss: 0.4845, Train Acc: 80.00%, Val Loss: 0.8750, Val Acc: 50.00%\n",
      "Starting epoch 26\n",
      "Train Loss: 0.5059, Train Acc: 75.00%, Val Loss: 0.8492, Val Acc: 50.00%\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Starting epoch 27\n",
      "Train Loss: 0.8388, Train Acc: 72.50%, Val Loss: 0.8649, Val Acc: 50.00%\n",
      "Early stopping triggered.\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss: 1.7522, Train Acc: 50.00%, Val Loss: 0.5401, Val Acc: 80.00%\n",
      "Starting epoch 2\n",
      "Train Loss: 0.7251, Train Acc: 70.00%, Val Loss: 0.6709, Val Acc: 90.00%\n",
      "Starting epoch 3\n",
      "Train Loss: 0.7520, Train Acc: 72.50%, Val Loss: 0.5682, Val Acc: 70.00%\n",
      "Starting epoch 4\n",
      "Train Loss: 0.6255, Train Acc: 77.50%, Val Loss: 0.7201, Val Acc: 70.00%\n",
      "Starting epoch 5\n",
      "Train Loss: 0.9520, Train Acc: 52.50%, Val Loss: 5.1463, Val Acc: 30.00%\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Starting epoch 6\n",
      "Train Loss: 1.2757, Train Acc: 55.00%, Val Loss: 2.1851, Val Acc: 30.00%\n",
      "Starting epoch 7\n",
      "Train Loss: 0.7386, Train Acc: 65.00%, Val Loss: 0.5278, Val Acc: 70.00%\n",
      "Starting epoch 8\n",
      "Train Loss: 0.8467, Train Acc: 65.00%, Val Loss: 0.4241, Val Acc: 70.00%\n",
      "Starting epoch 9\n",
      "Train Loss: 0.6260, Train Acc: 72.50%, Val Loss: 0.4355, Val Acc: 70.00%\n",
      "Starting epoch 10\n",
      "Train Loss: 0.6588, Train Acc: 70.00%, Val Loss: 0.4902, Val Acc: 70.00%\n",
      "Starting epoch 11\n",
      "Train Loss: 0.3862, Train Acc: 82.50%, Val Loss: 0.5391, Val Acc: 70.00%\n",
      "Starting epoch 12\n",
      "Train Loss: 0.5266, Train Acc: 75.00%, Val Loss: 0.5729, Val Acc: 70.00%\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Starting epoch 13\n",
      "Train Loss: 0.5620, Train Acc: 70.00%, Val Loss: 0.5997, Val Acc: 70.00%\n",
      "Starting epoch 14\n",
      "Train Loss: 0.5099, Train Acc: 85.00%, Val Loss: 0.5994, Val Acc: 70.00%\n",
      "Starting epoch 15\n",
      "Train Loss: 0.6221, Train Acc: 77.50%, Val Loss: 0.5843, Val Acc: 70.00%\n",
      "Starting epoch 16\n",
      "Train Loss: 0.4820, Train Acc: 82.50%, Val Loss: 0.6060, Val Acc: 70.00%\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Starting epoch 17\n",
      "Train Loss: 0.4729, Train Acc: 77.50%, Val Loss: 0.6079, Val Acc: 70.00%\n",
      "Starting epoch 18\n",
      "Train Loss: 0.4855, Train Acc: 80.00%, Val Loss: 0.6081, Val Acc: 70.00%\n",
      "Starting epoch 19\n",
      "Train Loss: 0.6919, Train Acc: 70.00%, Val Loss: 0.5621, Val Acc: 70.00%\n",
      "Starting epoch 20\n",
      "Train Loss: 0.5390, Train Acc: 75.00%, Val Loss: 0.5475, Val Acc: 70.00%\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Starting epoch 21\n",
      "Train Loss: 0.6588, Train Acc: 72.50%, Val Loss: 0.5761, Val Acc: 70.00%\n",
      "Starting epoch 22\n",
      "Train Loss: 0.5303, Train Acc: 77.50%, Val Loss: 0.5623, Val Acc: 70.00%\n",
      "Starting epoch 23\n",
      "Train Loss: 0.7015, Train Acc: 72.50%, Val Loss: 0.5680, Val Acc: 70.00%\n",
      "Starting epoch 24\n",
      "Train Loss: 0.8132, Train Acc: 70.00%, Val Loss: 0.5585, Val Acc: 70.00%\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Starting epoch 25\n",
      "Train Loss: 0.5592, Train Acc: 72.50%, Val Loss: 0.5821, Val Acc: 70.00%\n",
      "Starting epoch 26\n",
      "Train Loss: 0.6701, Train Acc: 82.50%, Val Loss: 0.6062, Val Acc: 70.00%\n",
      "Starting epoch 27\n",
      "Train Loss: 0.4811, Train Acc: 85.00%, Val Loss: 0.5740, Val Acc: 70.00%\n",
      "Starting epoch 28\n",
      "Train Loss: 0.5432, Train Acc: 80.00%, Val Loss: 0.5509, Val Acc: 70.00%\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Starting epoch 29\n",
      "Train Loss: 0.4971, Train Acc: 82.50%, Val Loss: 0.5387, Val Acc: 70.00%\n",
      "Starting epoch 30\n",
      "Train Loss: 0.6188, Train Acc: 77.50%, Val Loss: 0.5853, Val Acc: 70.00%\n",
      "Starting epoch 31\n",
      "Train Loss: 0.5893, Train Acc: 75.00%, Val Loss: 0.5525, Val Acc: 70.00%\n",
      "Starting epoch 32\n",
      "Train Loss: 0.6077, Train Acc: 75.00%, Val Loss: 0.5862, Val Acc: 70.00%\n",
      "Starting epoch 33\n",
      "Train Loss: 0.5348, Train Acc: 85.00%, Val Loss: 0.5569, Val Acc: 70.00%\n",
      "Early stopping triggered.\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss: 2.4388, Train Acc: 50.00%, Val Loss: 26.7200, Val Acc: 30.00%\n",
      "Starting epoch 2\n",
      "Train Loss: 1.0913, Train Acc: 50.00%, Val Loss: 0.8265, Val Acc: 70.00%\n",
      "Starting epoch 3\n",
      "Train Loss: 0.7277, Train Acc: 75.00%, Val Loss: 0.8859, Val Acc: 70.00%\n",
      "Starting epoch 4\n",
      "Train Loss: 1.3608, Train Acc: 52.50%, Val Loss: 0.6071, Val Acc: 60.00%\n",
      "Starting epoch 5\n",
      "Train Loss: 1.2461, Train Acc: 55.00%, Val Loss: 0.3866, Val Acc: 80.00%\n",
      "Starting epoch 6\n",
      "Train Loss: 0.6128, Train Acc: 67.50%, Val Loss: 0.8457, Val Acc: 60.00%\n",
      "Starting epoch 7\n",
      "Train Loss: 0.7070, Train Acc: 65.00%, Val Loss: 0.7588, Val Acc: 60.00%\n",
      "Starting epoch 8\n",
      "Train Loss: 0.6430, Train Acc: 75.00%, Val Loss: 0.9805, Val Acc: 60.00%\n",
      "Starting epoch 9\n",
      "Train Loss: 0.5164, Train Acc: 77.50%, Val Loss: 0.4190, Val Acc: 90.00%\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Starting epoch 10\n",
      "Train Loss: 0.5156, Train Acc: 82.50%, Val Loss: 0.5214, Val Acc: 80.00%\n",
      "Starting epoch 11\n",
      "Train Loss: 0.3953, Train Acc: 82.50%, Val Loss: 0.5945, Val Acc: 70.00%\n",
      "Starting epoch 12\n",
      "Train Loss: 0.7726, Train Acc: 72.50%, Val Loss: 0.5427, Val Acc: 80.00%\n",
      "Starting epoch 13\n",
      "Train Loss: 0.5957, Train Acc: 70.00%, Val Loss: 0.5163, Val Acc: 80.00%\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Starting epoch 14\n",
      "Train Loss: 0.5841, Train Acc: 77.50%, Val Loss: 0.5179, Val Acc: 80.00%\n",
      "Starting epoch 15\n",
      "Train Loss: 0.5182, Train Acc: 77.50%, Val Loss: 0.4927, Val Acc: 80.00%\n",
      "Starting epoch 16\n",
      "Train Loss: 0.4404, Train Acc: 82.50%, Val Loss: 0.4988, Val Acc: 80.00%\n",
      "Starting epoch 17\n",
      "Train Loss: 0.5890, Train Acc: 80.00%, Val Loss: 0.5118, Val Acc: 80.00%\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Starting epoch 18\n",
      "Train Loss: 0.5862, Train Acc: 82.50%, Val Loss: 0.4974, Val Acc: 80.00%\n",
      "Starting epoch 19\n",
      "Train Loss: 0.6619, Train Acc: 62.50%, Val Loss: 0.4971, Val Acc: 80.00%\n",
      "Starting epoch 20\n",
      "Train Loss: 0.6120, Train Acc: 77.50%, Val Loss: 0.5002, Val Acc: 80.00%\n",
      "Starting epoch 21\n",
      "Train Loss: 0.6818, Train Acc: 77.50%, Val Loss: 0.4953, Val Acc: 80.00%\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Starting epoch 22\n",
      "Train Loss: 0.4319, Train Acc: 80.00%, Val Loss: 0.4740, Val Acc: 80.00%\n",
      "Starting epoch 23\n",
      "Train Loss: 0.5037, Train Acc: 80.00%, Val Loss: 0.4701, Val Acc: 80.00%\n",
      "Starting epoch 24\n",
      "Train Loss: 0.6071, Train Acc: 75.00%, Val Loss: 0.4818, Val Acc: 80.00%\n",
      "Starting epoch 25\n",
      "Train Loss: 0.5434, Train Acc: 77.50%, Val Loss: 0.4762, Val Acc: 80.00%\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Starting epoch 26\n",
      "Train Loss: 0.5816, Train Acc: 80.00%, Val Loss: 0.4683, Val Acc: 80.00%\n",
      "Starting epoch 27\n",
      "Train Loss: 0.5653, Train Acc: 75.00%, Val Loss: 0.4891, Val Acc: 80.00%\n",
      "Starting epoch 28\n",
      "Train Loss: 0.8709, Train Acc: 67.50%, Val Loss: 0.4890, Val Acc: 80.00%\n",
      "Starting epoch 29\n",
      "Train Loss: 0.6590, Train Acc: 75.00%, Val Loss: 0.5038, Val Acc: 80.00%\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Starting epoch 30\n",
      "Train Loss: 0.7171, Train Acc: 70.00%, Val Loss: 0.4988, Val Acc: 80.00%\n",
      "Early stopping triggered.\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss: 1.5375, Train Acc: 50.00%, Val Loss: 1.5912, Val Acc: 80.00%\n",
      "Starting epoch 2\n",
      "Train Loss: 0.9562, Train Acc: 62.50%, Val Loss: 0.6233, Val Acc: 80.00%\n",
      "Starting epoch 3\n",
      "Train Loss: 0.9979, Train Acc: 67.50%, Val Loss: 0.5266, Val Acc: 80.00%\n",
      "Starting epoch 4\n",
      "Train Loss: 1.0656, Train Acc: 72.50%, Val Loss: 0.8319, Val Acc: 70.00%\n",
      "Starting epoch 5\n",
      "Train Loss: 1.0179, Train Acc: 60.00%, Val Loss: 0.6611, Val Acc: 60.00%\n",
      "Starting epoch 6\n",
      "Train Loss: 1.0314, Train Acc: 57.50%, Val Loss: 1.2432, Val Acc: 50.00%\n",
      "Starting epoch 7\n",
      "Train Loss: 0.5333, Train Acc: 80.00%, Val Loss: 0.5542, Val Acc: 80.00%\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Starting epoch 8\n",
      "Train Loss: 0.6825, Train Acc: 80.00%, Val Loss: 0.6551, Val Acc: 80.00%\n",
      "Starting epoch 9\n",
      "Train Loss: 0.9401, Train Acc: 75.00%, Val Loss: 0.7643, Val Acc: 70.00%\n",
      "Starting epoch 10\n",
      "Train Loss: 0.5844, Train Acc: 80.00%, Val Loss: 0.8431, Val Acc: 60.00%\n",
      "Starting epoch 11\n",
      "Train Loss: 0.5019, Train Acc: 77.50%, Val Loss: 0.7904, Val Acc: 60.00%\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Starting epoch 12\n",
      "Train Loss: 0.4042, Train Acc: 87.50%, Val Loss: 0.7595, Val Acc: 60.00%\n",
      "Starting epoch 13\n",
      "Train Loss: 0.6702, Train Acc: 72.50%, Val Loss: 0.7649, Val Acc: 70.00%\n",
      "Starting epoch 14\n",
      "Train Loss: 0.6351, Train Acc: 67.50%, Val Loss: 0.7191, Val Acc: 70.00%\n",
      "Starting epoch 15\n",
      "Train Loss: 0.5689, Train Acc: 77.50%, Val Loss: 0.6864, Val Acc: 80.00%\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Starting epoch 16\n",
      "Train Loss: 0.5970, Train Acc: 80.00%, Val Loss: 0.6572, Val Acc: 80.00%\n",
      "Starting epoch 17\n",
      "Train Loss: 0.8604, Train Acc: 60.00%, Val Loss: 0.6915, Val Acc: 70.00%\n",
      "Starting epoch 18\n",
      "Train Loss: 0.5912, Train Acc: 85.00%, Val Loss: 0.6779, Val Acc: 70.00%\n",
      "Starting epoch 19\n",
      "Train Loss: 0.5057, Train Acc: 80.00%, Val Loss: 0.6842, Val Acc: 70.00%\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Starting epoch 20\n",
      "Train Loss: 0.8615, Train Acc: 67.50%, Val Loss: 0.6775, Val Acc: 80.00%\n",
      "Starting epoch 21\n",
      "Train Loss: 0.6979, Train Acc: 70.00%, Val Loss: 0.6511, Val Acc: 80.00%\n",
      "Starting epoch 22\n",
      "Train Loss: 0.4731, Train Acc: 85.00%, Val Loss: 0.6787, Val Acc: 80.00%\n",
      "Starting epoch 23\n",
      "Train Loss: 0.5395, Train Acc: 72.50%, Val Loss: 0.6956, Val Acc: 70.00%\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Starting epoch 24\n",
      "Train Loss: 0.5640, Train Acc: 80.00%, Val Loss: 0.6637, Val Acc: 80.00%\n",
      "Starting epoch 25\n",
      "Train Loss: 0.3333, Train Acc: 82.50%, Val Loss: 0.6635, Val Acc: 80.00%\n",
      "Starting epoch 26\n",
      "Train Loss: 0.3723, Train Acc: 80.00%, Val Loss: 0.6497, Val Acc: 80.00%\n",
      "Starting epoch 27\n",
      "Train Loss: 0.6395, Train Acc: 67.50%, Val Loss: 0.6169, Val Acc: 80.00%\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Starting epoch 28\n",
      "Train Loss: 0.5518, Train Acc: 77.50%, Val Loss: 0.6501, Val Acc: 80.00%\n",
      "Early stopping triggered.\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Train Loss: 0.9041, Train Acc: 55.00%, Val Loss: 1.4507, Val Acc: 80.00%\n",
      "Starting epoch 2\n",
      "Train Loss: 1.2103, Train Acc: 62.50%, Val Loss: 1.0120, Val Acc: 70.00%\n",
      "Starting epoch 3\n",
      "Train Loss: 1.2952, Train Acc: 70.00%, Val Loss: 0.9463, Val Acc: 70.00%\n",
      "Starting epoch 4\n",
      "Train Loss: 1.5580, Train Acc: 57.50%, Val Loss: 1.3866, Val Acc: 60.00%\n",
      "Starting epoch 5\n",
      "Train Loss: 1.3141, Train Acc: 75.00%, Val Loss: 1.0854, Val Acc: 70.00%\n",
      "Starting epoch 6\n",
      "Train Loss: 0.9174, Train Acc: 65.00%, Val Loss: 0.4760, Val Acc: 70.00%\n",
      "Starting epoch 7\n",
      "Train Loss: 1.1659, Train Acc: 62.50%, Val Loss: 0.6041, Val Acc: 70.00%\n",
      "Starting epoch 8\n",
      "Train Loss: 0.5681, Train Acc: 67.50%, Val Loss: 0.5461, Val Acc: 70.00%\n",
      "Starting epoch 9\n",
      "Train Loss: 0.5382, Train Acc: 75.00%, Val Loss: 0.9076, Val Acc: 60.00%\n",
      "Starting epoch 10\n",
      "Train Loss: 0.8545, Train Acc: 67.50%, Val Loss: 1.1519, Val Acc: 50.00%\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Starting epoch 11\n",
      "Train Loss: 0.7094, Train Acc: 67.50%, Val Loss: 0.8486, Val Acc: 50.00%\n",
      "Starting epoch 12\n",
      "Train Loss: 0.6805, Train Acc: 72.50%, Val Loss: 0.6219, Val Acc: 60.00%\n",
      "Starting epoch 13\n",
      "Train Loss: 0.4946, Train Acc: 75.00%, Val Loss: 0.5307, Val Acc: 70.00%\n",
      "Starting epoch 14\n",
      "Train Loss: 0.5417, Train Acc: 77.50%, Val Loss: 0.4733, Val Acc: 70.00%\n",
      "Starting epoch 15\n",
      "Train Loss: 0.6499, Train Acc: 77.50%, Val Loss: 0.5190, Val Acc: 70.00%\n",
      "Starting epoch 16\n",
      "Train Loss: 0.4321, Train Acc: 77.50%, Val Loss: 0.5937, Val Acc: 70.00%\n",
      "Starting epoch 17\n",
      "Train Loss: 0.3498, Train Acc: 82.50%, Val Loss: 0.6856, Val Acc: 70.00%\n",
      "Starting epoch 18\n",
      "Train Loss: 0.7912, Train Acc: 67.50%, Val Loss: 0.6184, Val Acc: 70.00%\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Starting epoch 19\n",
      "Train Loss: 0.6377, Train Acc: 77.50%, Val Loss: 0.5556, Val Acc: 70.00%\n",
      "Starting epoch 20\n",
      "Train Loss: 0.4871, Train Acc: 80.00%, Val Loss: 0.5378, Val Acc: 70.00%\n",
      "Starting epoch 21\n",
      "Train Loss: 0.4951, Train Acc: 72.50%, Val Loss: 0.5515, Val Acc: 70.00%\n",
      "Starting epoch 22\n",
      "Train Loss: 0.6304, Train Acc: 72.50%, Val Loss: 0.5061, Val Acc: 70.00%\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Starting epoch 23\n",
      "Train Loss: 0.4139, Train Acc: 77.50%, Val Loss: 0.5370, Val Acc: 70.00%\n",
      "Starting epoch 24\n",
      "Train Loss: 0.4458, Train Acc: 80.00%, Val Loss: 0.5678, Val Acc: 70.00%\n",
      "Starting epoch 25\n",
      "Train Loss: 0.4384, Train Acc: 77.50%, Val Loss: 0.5719, Val Acc: 70.00%\n",
      "Starting epoch 26\n",
      "Train Loss: 0.6465, Train Acc: 75.00%, Val Loss: 0.6043, Val Acc: 70.00%\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Starting epoch 27\n",
      "Train Loss: 0.4889, Train Acc: 77.50%, Val Loss: 0.5559, Val Acc: 70.00%\n",
      "Starting epoch 28\n",
      "Train Loss: 0.7004, Train Acc: 75.00%, Val Loss: 0.5529, Val Acc: 70.00%\n",
      "Starting epoch 29\n",
      "Train Loss: 0.5382, Train Acc: 77.50%, Val Loss: 0.5130, Val Acc: 70.00%\n",
      "Starting epoch 30\n",
      "Train Loss: 0.4955, Train Acc: 80.00%, Val Loss: 0.5235, Val Acc: 70.00%\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Starting epoch 31\n",
      "Train Loss: 0.5170, Train Acc: 72.50%, Val Loss: 0.5334, Val Acc: 70.00%\n",
      "Starting epoch 32\n",
      "Train Loss: 0.3970, Train Acc: 80.00%, Val Loss: 0.5755, Val Acc: 70.00%\n",
      "Starting epoch 33\n",
      "Train Loss: 0.3567, Train Acc: 92.50%, Val Loss: 0.5522, Val Acc: 70.00%\n",
      "Starting epoch 34\n",
      "Train Loss: 0.6133, Train Acc: 75.00%, Val Loss: 0.5578, Val Acc: 70.00%\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Starting epoch 35\n",
      "Train Loss: 0.3720, Train Acc: 87.50%, Val Loss: 0.5194, Val Acc: 70.00%\n",
      "Starting epoch 36\n",
      "Train Loss: 0.4252, Train Acc: 80.00%, Val Loss: 0.5206, Val Acc: 70.00%\n",
      "Starting epoch 37\n",
      "Train Loss: 0.6247, Train Acc: 70.00%, Val Loss: 0.5116, Val Acc: 70.00%\n",
      "Starting epoch 38\n",
      "Train Loss: 0.5274, Train Acc: 77.50%, Val Loss: 0.5106, Val Acc: 70.00%\n",
      "Starting epoch 39\n",
      "Train Loss: 0.5581, Train Acc: 67.50%, Val Loss: 0.5027, Val Acc: 70.00%\n",
      "Early stopping triggered.\n",
      "--------------------------------\n",
      "Best validation accuracy of 0.00% achieved, model saved as best_model.pth\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(trainset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_indices = [i for i, (_, label) in enumerate(trainset) if label in selected_classes]\n",
    "\n",
    "    N = 25  # Assuming we want at most N samples per selected class\n",
    "    class_counts = {label: 0 for label in selected_classes}\n",
    "    filtered_train_indices = []\n",
    "\n",
    "    for i in train_indices:\n",
    "        _, label = trainset[i]\n",
    "        if class_counts[label] < N:\n",
    "            filtered_train_indices.append(i)\n",
    "            class_counts[label] += 1\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    np.random.seed(42)  # Ensure reproducibility\n",
    "    np.random.shuffle(filtered_train_indices)  # Shuffle the indices\n",
    "    split = int(0.8 * len(filtered_train_indices))  # 80% of indices for training\n",
    "    train_idx, val_idx = filtered_train_indices[:split], filtered_train_indices[split:]\n",
    "    \n",
    "    train_subset = Subset(trainset, train_idx)\n",
    "    transformed_train_subset = TransformSubset(train_subset, transform=train_transform)\n",
    "\n",
    "    val_subset = Subset(trainset, val_idx)\n",
    "    transformed_val_subset = TransformSubset(val_subset, transform=test_transform)\n",
    "    \n",
    "    train_loader = DataLoader(transformed_train_subset, batch_size=5, shuffle=True)\n",
    "    val_loader = DataLoader(transformed_val_subset, batch_size=5, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # Init the neural network\n",
    "    model = CustomCNN().to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.005) \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 25\n",
    "\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Perform training and validation\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, criterion)\n",
    "        val_loss, val_accuracy = validate(model, device, val_loader, criterion)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Save the model if it has the best val accuracy so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_weights = model.state_dict().copy()  # Save the best model weights\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break  # Stop training if no improvement\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "            \n",
    "    print('--------------------------------')\n",
    "    \n",
    "# Save the best model weights\n",
    "torch.save(best_model_weights, 'best_model.pth')\n",
    "print(f'Best validation accuracy of {best_val_accuracy:.2f}% achieved, model saved as best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomCNN()\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = [i for i, (_, label) in enumerate(testset) if label in selected_classes]\n",
    "N = 2000  # Assuming we want at most N samples per selected class\n",
    "class_counts = {label: 0 for label in selected_classes}\n",
    "filtered_train_indices = []\n",
    "\n",
    "for i in test_indices:\n",
    "    _, label = testset[i]\n",
    "    if class_counts[label] < N:\n",
    "        filtered_train_indices.append(i)\n",
    "        class_counts[label] += 1\n",
    "\n",
    "test_subset = Subset(testset, filtered_train_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3422, Test Accuracy: 86.30%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Assuming the test_loader is already defined\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluation\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# No gradient is needed for evaluation\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "avg_loss = test_loss / len(test_loader)\n",
    "accuracy = 100. * correct / total\n",
    "\n",
    "print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f10e0bbdcb8a5227530b5aed21b374411d613e5cbe3bc0662ba6adc52ae7b0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
