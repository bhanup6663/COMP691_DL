{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJCVTgQGpOyI6cTWQtEyc0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanup6663/COMP691_DL/blob/main/Challange2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNet"
      ],
      "metadata": {
        "id": "XyJUJLRtG3CQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J-IyPd0hBD_v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset,DataLoader, Subset, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qRk2IWIBZ80",
        "outputId": "e1ab5f70-c919-4d1f-9852-4fdc6dfac0fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "])"
      ],
      "metadata": {
        "id": "jikRIEHnBa7r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QFus1JDBeCY",
        "outputId": "a22fbbad-5fe1-4031-d03f-5e8f007cc59f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12580727.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_classes = np.random.choice(range(10), 2, replace=False)\n",
        "print(selected_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8_L_PIyBg94",
        "outputId": "e70715ce-6d2b-49a6-ca70-d90b440509a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "num_classes = 10\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=dropout_rate),\n",
        "    nn.Linear(model.classifier[1].in_features, num_classes)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XagoSDYKBj1U",
        "outputId": "dcddb434-6e47-4d26-8a03-2711d96b7358"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 134MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_folds = 5\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "K6IHtXjiBnWT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_accuracy = 0\n",
        "best_model_weights = None"
      ],
      "metadata": {
        "id": "BAzUjiOVBqC8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "P33pJEWKBssv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, device, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, targets) in enumerate(val_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "iyuG3D48Bvmb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformSubset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "nbL1H3USBxuj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(trainset)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    train_indices = [i for i, (_, label) in enumerate(trainset) if label in selected_classes]\n",
        "\n",
        "    N = 25\n",
        "    class_counts = {label: 0 for label in selected_classes}\n",
        "    filtered_train_indices = []\n",
        "\n",
        "    for i in train_indices:\n",
        "        _, label = trainset[i]\n",
        "        if class_counts[label] < N:\n",
        "            filtered_train_indices.append(i)\n",
        "            class_counts[label] += 1\n",
        "\n",
        "\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(filtered_train_indices)\n",
        "    split = int(0.8 * len(filtered_train_indices))\n",
        "    train_idx, val_idx = filtered_train_indices[:split], filtered_train_indices[split:]\n",
        "\n",
        "    train_subset = Subset(trainset, train_idx)\n",
        "    transformed_train_subset = TransformSubset(train_subset, transform=train_transform)\n",
        "\n",
        "    val_subset = Subset(trainset, val_idx)\n",
        "    transformed_val_subset = TransformSubset(val_subset, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(transformed_train_subset, batch_size=5, shuffle=True)\n",
        "    val_loader = DataLoader(transformed_val_subset, batch_size=5, shuffle=False)\n",
        "\n",
        "\n",
        "    # Init the neural network\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 25\n",
        "\n",
        "\n",
        "    # Run the training loop for defined number of epochs\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "        # Perform training and validation\n",
        "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, criterion)\n",
        "        val_loss, val_accuracy = validate(model, device, val_loader, criterion)\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Save the model if it has the best val accuracy so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_weights = model.state_dict().copy()  # Save the best model weights\n",
        "            patience_counter = 0  # Reset patience\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break  # Stop training if no improvement\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print('--------------------------------')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_weights, 'c2_mobilenet_best_model.pth')\n",
        "print(f'Best validation accuracy of {best_val_accuracy:.2f}% achieved, model saved as c2_mobilenet_best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMMnEM8QB0fL",
        "outputId": "6a900273-db9b-450d-d2a4-21ac0c9d89ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Train Loss: 2.7680, Train Acc: 5.00%, Val Loss: 2.3530, Val Acc: 0.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 2.4923, Train Acc: 15.00%, Val Loss: 2.2751, Val Acc: 10.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 2.4216, Train Acc: 20.00%, Val Loss: 2.1477, Val Acc: 20.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 2.2317, Train Acc: 20.00%, Val Loss: 2.0767, Val Acc: 20.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 2.1664, Train Acc: 20.00%, Val Loss: 2.0892, Val Acc: 20.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 2.3874, Train Acc: 20.00%, Val Loss: 2.0064, Val Acc: 30.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 2.1416, Train Acc: 20.00%, Val Loss: 2.0487, Val Acc: 30.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 2.0813, Train Acc: 25.00%, Val Loss: 1.9818, Val Acc: 20.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 1.8639, Train Acc: 32.50%, Val Loss: 1.7933, Val Acc: 60.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 1.8663, Train Acc: 35.00%, Val Loss: 1.8812, Val Acc: 40.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 1.6177, Train Acc: 50.00%, Val Loss: 1.8508, Val Acc: 40.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 1.6171, Train Acc: 52.50%, Val Loss: 1.7509, Val Acc: 60.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 1.5998, Train Acc: 42.50%, Val Loss: 1.6495, Val Acc: 70.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 1.5726, Train Acc: 45.00%, Val Loss: 1.4867, Val Acc: 70.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 1.3365, Train Acc: 55.00%, Val Loss: 1.5614, Val Acc: 60.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 1.4261, Train Acc: 57.50%, Val Loss: 1.5291, Val Acc: 60.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 1.4954, Train Acc: 45.00%, Val Loss: 1.6384, Val Acc: 60.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 1.3807, Train Acc: 50.00%, Val Loss: 1.5647, Val Acc: 60.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 1.4313, Train Acc: 55.00%, Val Loss: 1.4169, Val Acc: 70.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 1.4421, Train Acc: 55.00%, Val Loss: 1.3667, Val Acc: 70.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 1.3418, Train Acc: 55.00%, Val Loss: 1.2638, Val Acc: 70.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 1.5378, Train Acc: 42.50%, Val Loss: 1.5678, Val Acc: 60.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 1.3124, Train Acc: 52.50%, Val Loss: 1.4419, Val Acc: 70.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 1.3402, Train Acc: 62.50%, Val Loss: 1.4936, Val Acc: 60.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 1.4036, Train Acc: 57.50%, Val Loss: 1.5413, Val Acc: 70.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 1.5681, Train Acc: 40.00%, Val Loss: 1.4557, Val Acc: 70.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 1.1661, Train Acc: 57.50%, Val Loss: 1.4269, Val Acc: 70.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 1.4306, Train Acc: 52.50%, Val Loss: 1.3984, Val Acc: 70.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 1.3424, Train Acc: 57.50%, Val Loss: 1.4416, Val Acc: 70.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 1.7059, Train Acc: 42.50%, Val Loss: 1.3009, Val Acc: 80.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 1.5668, Train Acc: 35.00%, Val Loss: 1.4037, Val Acc: 70.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 1.4849, Train Acc: 50.00%, Val Loss: 1.3281, Val Acc: 70.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 1.4350, Train Acc: 50.00%, Val Loss: 1.4069, Val Acc: 80.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 1.3939, Train Acc: 52.50%, Val Loss: 1.3846, Val Acc: 80.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 1.4950, Train Acc: 47.50%, Val Loss: 1.4336, Val Acc: 70.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 1.1827, Train Acc: 57.50%, Val Loss: 1.4685, Val Acc: 70.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 1.2770, Train Acc: 62.50%, Val Loss: 1.4466, Val Acc: 80.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 1.5217, Train Acc: 40.00%, Val Loss: 1.3570, Val Acc: 80.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 1.3527, Train Acc: 50.00%, Val Loss: 1.4545, Val Acc: 70.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 1.5397, Train Acc: 47.50%, Val Loss: 1.3976, Val Acc: 70.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 1.3482, Train Acc: 52.50%, Val Loss: 1.3819, Val Acc: 80.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 1.4049, Train Acc: 52.50%, Val Loss: 1.4418, Val Acc: 70.00%\n",
            "Starting epoch 43\n",
            "Train Loss: 1.6100, Train Acc: 37.50%, Val Loss: 1.4643, Val Acc: 70.00%\n",
            "Starting epoch 44\n",
            "Train Loss: 1.5491, Train Acc: 50.00%, Val Loss: 1.5819, Val Acc: 70.00%\n",
            "Starting epoch 45\n",
            "Train Loss: 1.1978, Train Acc: 62.50%, Val Loss: 1.3938, Val Acc: 70.00%\n",
            "Starting epoch 46\n",
            "Train Loss: 1.3874, Train Acc: 52.50%, Val Loss: 1.4826, Val Acc: 60.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 1.3424, Train Acc: 55.00%, Val Loss: 1.3639, Val Acc: 70.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 1.2646, Train Acc: 65.00%, Val Loss: 1.3268, Val Acc: 70.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 1.2795, Train Acc: 57.50%, Val Loss: 1.3929, Val Acc: 80.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 1.2192, Train Acc: 62.50%, Val Loss: 1.3869, Val Acc: 60.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 1.2674, Train Acc: 55.00%, Val Loss: 1.3216, Val Acc: 60.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.9723, Train Acc: 75.00%, Val Loss: 1.2866, Val Acc: 60.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.9644, Train Acc: 65.00%, Val Loss: 1.2377, Val Acc: 60.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 1.1177, Train Acc: 57.50%, Val Loss: 1.3815, Val Acc: 60.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.9729, Train Acc: 60.00%, Val Loss: 1.3107, Val Acc: 60.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.9949, Train Acc: 67.50%, Val Loss: 0.9801, Val Acc: 80.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 1.0277, Train Acc: 70.00%, Val Loss: 1.0681, Val Acc: 70.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.9415, Train Acc: 65.00%, Val Loss: 1.1574, Val Acc: 80.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.9189, Train Acc: 57.50%, Val Loss: 1.0978, Val Acc: 80.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.8336, Train Acc: 70.00%, Val Loss: 1.2487, Val Acc: 50.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.9002, Train Acc: 70.00%, Val Loss: 1.1160, Val Acc: 60.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.9457, Train Acc: 57.50%, Val Loss: 1.1084, Val Acc: 60.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.9184, Train Acc: 72.50%, Val Loss: 0.8999, Val Acc: 90.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.7909, Train Acc: 67.50%, Val Loss: 1.0081, Val Acc: 90.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.8692, Train Acc: 57.50%, Val Loss: 1.0776, Val Acc: 90.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.7953, Train Acc: 67.50%, Val Loss: 1.1949, Val Acc: 50.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.8505, Train Acc: 72.50%, Val Loss: 1.1847, Val Acc: 60.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.8059, Train Acc: 75.00%, Val Loss: 1.4188, Val Acc: 50.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.9039, Train Acc: 57.50%, Val Loss: 1.0464, Val Acc: 60.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.9692, Train Acc: 62.50%, Val Loss: 1.1443, Val Acc: 70.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 1.1078, Train Acc: 47.50%, Val Loss: 0.9004, Val Acc: 90.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.9761, Train Acc: 67.50%, Val Loss: 1.1114, Val Acc: 70.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.7924, Train Acc: 70.00%, Val Loss: 1.0869, Val Acc: 80.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.8269, Train Acc: 62.50%, Val Loss: 1.1092, Val Acc: 80.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.7867, Train Acc: 72.50%, Val Loss: 1.0687, Val Acc: 70.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.9084, Train Acc: 67.50%, Val Loss: 1.1057, Val Acc: 60.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.9413, Train Acc: 62.50%, Val Loss: 1.2456, Val Acc: 50.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.9247, Train Acc: 67.50%, Val Loss: 1.1147, Val Acc: 80.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.9107, Train Acc: 65.00%, Val Loss: 1.0824, Val Acc: 80.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.8363, Train Acc: 75.00%, Val Loss: 1.3457, Val Acc: 70.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.9523, Train Acc: 67.50%, Val Loss: 1.2615, Val Acc: 70.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.8657, Train Acc: 72.50%, Val Loss: 1.2481, Val Acc: 60.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 1.0271, Train Acc: 60.00%, Val Loss: 1.2637, Val Acc: 60.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 1.0297, Train Acc: 57.50%, Val Loss: 1.0830, Val Acc: 70.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 0.9702, Train Acc: 62.50%, Val Loss: 1.0683, Val Acc: 70.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 0.8442, Train Acc: 67.50%, Val Loss: 1.0363, Val Acc: 80.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 1.0129, Train Acc: 57.50%, Val Loss: 1.2209, Val Acc: 60.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 0.9166, Train Acc: 57.50%, Val Loss: 1.2343, Val Acc: 50.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 0.7752, Train Acc: 75.00%, Val Loss: 1.1722, Val Acc: 60.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.7543, Train Acc: 72.50%, Val Loss: 1.1128, Val Acc: 70.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 1.0134, Train Acc: 60.00%, Val Loss: 1.1739, Val Acc: 60.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.8753, Train Acc: 65.00%, Val Loss: 1.1253, Val Acc: 50.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.8084, Train Acc: 60.00%, Val Loss: 1.0566, Val Acc: 60.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.8457, Train Acc: 62.50%, Val Loss: 1.0211, Val Acc: 60.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.8857, Train Acc: 57.50%, Val Loss: 1.0085, Val Acc: 70.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.7110, Train Acc: 75.00%, Val Loss: 0.9300, Val Acc: 80.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.8637, Train Acc: 52.50%, Val Loss: 0.9621, Val Acc: 70.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.8261, Train Acc: 62.50%, Val Loss: 1.1034, Val Acc: 50.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.7764, Train Acc: 67.50%, Val Loss: 0.9385, Val Acc: 70.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.7388, Train Acc: 72.50%, Val Loss: 0.9849, Val Acc: 50.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.5894, Train Acc: 77.50%, Val Loss: 0.9403, Val Acc: 60.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.7141, Train Acc: 67.50%, Val Loss: 0.8980, Val Acc: 60.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.8350, Train Acc: 62.50%, Val Loss: 0.8922, Val Acc: 80.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.8106, Train Acc: 70.00%, Val Loss: 0.7623, Val Acc: 70.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.8068, Train Acc: 57.50%, Val Loss: 1.0504, Val Acc: 60.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.7009, Train Acc: 72.50%, Val Loss: 1.0581, Val Acc: 50.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.6712, Train Acc: 70.00%, Val Loss: 1.0031, Val Acc: 60.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.6175, Train Acc: 77.50%, Val Loss: 1.0447, Val Acc: 60.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.7472, Train Acc: 65.00%, Val Loss: 0.9736, Val Acc: 50.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.8717, Train Acc: 52.50%, Val Loss: 0.9362, Val Acc: 80.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.8401, Train Acc: 60.00%, Val Loss: 0.9532, Val Acc: 60.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.7786, Train Acc: 67.50%, Val Loss: 0.9989, Val Acc: 60.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.5560, Train Acc: 77.50%, Val Loss: 0.8671, Val Acc: 80.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.7077, Train Acc: 60.00%, Val Loss: 0.9652, Val Acc: 50.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.7629, Train Acc: 65.00%, Val Loss: 1.0369, Val Acc: 60.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.7642, Train Acc: 65.00%, Val Loss: 0.9214, Val Acc: 70.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.8669, Train Acc: 60.00%, Val Loss: 0.9429, Val Acc: 70.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.6691, Train Acc: 70.00%, Val Loss: 1.0395, Val Acc: 60.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.6575, Train Acc: 72.50%, Val Loss: 0.8675, Val Acc: 60.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.6590, Train Acc: 77.50%, Val Loss: 0.9005, Val Acc: 60.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.8249, Train Acc: 62.50%, Val Loss: 1.0304, Val Acc: 60.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.6523, Train Acc: 72.50%, Val Loss: 0.9768, Val Acc: 40.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.8491, Train Acc: 67.50%, Val Loss: 1.0537, Val Acc: 70.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.8296, Train Acc: 60.00%, Val Loss: 1.0732, Val Acc: 40.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 0.7990, Train Acc: 65.00%, Val Loss: 1.0124, Val Acc: 50.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 0.7714, Train Acc: 72.50%, Val Loss: 0.9884, Val Acc: 60.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 0.7979, Train Acc: 72.50%, Val Loss: 1.0523, Val Acc: 50.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 0.6760, Train Acc: 70.00%, Val Loss: 0.9811, Val Acc: 50.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 0.6555, Train Acc: 67.50%, Val Loss: 0.9499, Val Acc: 60.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 0.8159, Train Acc: 57.50%, Val Loss: 0.9707, Val Acc: 60.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.8127, Train Acc: 70.00%, Val Loss: 0.8454, Val Acc: 70.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 0.8262, Train Acc: 65.00%, Val Loss: 1.0740, Val Acc: 60.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.6659, Train Acc: 67.50%, Val Loss: 0.8965, Val Acc: 60.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.7349, Train Acc: 57.50%, Val Loss: 0.9233, Val Acc: 60.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.8583, Train Acc: 62.50%, Val Loss: 0.8818, Val Acc: 50.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.8170, Train Acc: 52.50%, Val Loss: 0.9119, Val Acc: 50.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.8481, Train Acc: 52.50%, Val Loss: 0.8827, Val Acc: 70.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.6582, Train Acc: 62.50%, Val Loss: 0.9303, Val Acc: 80.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.7078, Train Acc: 67.50%, Val Loss: 0.9613, Val Acc: 70.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.7117, Train Acc: 67.50%, Val Loss: 0.9979, Val Acc: 50.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.6821, Train Acc: 65.00%, Val Loss: 0.8612, Val Acc: 70.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.7650, Train Acc: 62.50%, Val Loss: 0.8986, Val Acc: 50.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.6579, Train Acc: 67.50%, Val Loss: 0.9580, Val Acc: 60.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.6771, Train Acc: 70.00%, Val Loss: 0.9219, Val Acc: 70.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.6541, Train Acc: 75.00%, Val Loss: 1.0045, Val Acc: 40.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.5684, Train Acc: 82.50%, Val Loss: 1.1400, Val Acc: 60.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.8196, Train Acc: 62.50%, Val Loss: 0.9752, Val Acc: 50.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.7583, Train Acc: 75.00%, Val Loss: 0.9293, Val Acc: 60.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.6605, Train Acc: 62.50%, Val Loss: 0.8913, Val Acc: 60.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.6747, Train Acc: 72.50%, Val Loss: 0.9186, Val Acc: 50.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.6274, Train Acc: 62.50%, Val Loss: 1.0840, Val Acc: 40.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.8238, Train Acc: 52.50%, Val Loss: 1.0006, Val Acc: 30.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.6778, Train Acc: 65.00%, Val Loss: 0.8994, Val Acc: 50.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.7187, Train Acc: 62.50%, Val Loss: 0.9542, Val Acc: 60.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.6442, Train Acc: 67.50%, Val Loss: 0.9263, Val Acc: 50.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.7167, Train Acc: 67.50%, Val Loss: 0.9218, Val Acc: 60.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 0.6485, Train Acc: 72.50%, Val Loss: 0.9603, Val Acc: 50.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.7115, Train Acc: 65.00%, Val Loss: 0.9625, Val Acc: 50.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 0.6885, Train Acc: 65.00%, Val Loss: 0.9344, Val Acc: 60.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.6711, Train Acc: 75.00%, Val Loss: 0.8422, Val Acc: 70.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.7102, Train Acc: 62.50%, Val Loss: 0.9200, Val Acc: 50.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.7850, Train Acc: 60.00%, Val Loss: 0.9379, Val Acc: 80.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.6130, Train Acc: 75.00%, Val Loss: 0.8499, Val Acc: 70.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.6151, Train Acc: 65.00%, Val Loss: 0.8833, Val Acc: 60.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.6262, Train Acc: 72.50%, Val Loss: 0.8647, Val Acc: 60.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.7431, Train Acc: 70.00%, Val Loss: 1.0052, Val Acc: 70.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.7032, Train Acc: 65.00%, Val Loss: 0.9582, Val Acc: 60.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.6605, Train Acc: 65.00%, Val Loss: 0.7762, Val Acc: 80.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.7216, Train Acc: 65.00%, Val Loss: 0.8091, Val Acc: 70.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.6722, Train Acc: 62.50%, Val Loss: 0.7882, Val Acc: 70.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.7839, Train Acc: 60.00%, Val Loss: 0.7586, Val Acc: 70.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.5991, Train Acc: 70.00%, Val Loss: 0.7605, Val Acc: 70.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.5818, Train Acc: 72.50%, Val Loss: 0.7539, Val Acc: 60.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.6833, Train Acc: 70.00%, Val Loss: 0.7971, Val Acc: 60.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.6360, Train Acc: 75.00%, Val Loss: 0.9203, Val Acc: 60.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.6630, Train Acc: 60.00%, Val Loss: 0.7695, Val Acc: 60.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.7184, Train Acc: 70.00%, Val Loss: 0.7899, Val Acc: 70.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.7908, Train Acc: 52.50%, Val Loss: 0.8743, Val Acc: 70.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.5582, Train Acc: 70.00%, Val Loss: 0.8248, Val Acc: 70.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.7401, Train Acc: 60.00%, Val Loss: 0.8489, Val Acc: 70.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.7655, Train Acc: 60.00%, Val Loss: 0.8668, Val Acc: 70.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.8186, Train Acc: 55.00%, Val Loss: 0.7595, Val Acc: 70.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.8874, Train Acc: 52.50%, Val Loss: 0.9784, Val Acc: 70.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.6637, Train Acc: 70.00%, Val Loss: 0.7487, Val Acc: 70.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.7211, Train Acc: 70.00%, Val Loss: 0.8390, Val Acc: 90.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.6564, Train Acc: 72.50%, Val Loss: 0.8402, Val Acc: 80.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.6536, Train Acc: 70.00%, Val Loss: 0.7510, Val Acc: 80.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.7358, Train Acc: 60.00%, Val Loss: 0.7029, Val Acc: 80.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.6226, Train Acc: 67.50%, Val Loss: 0.6937, Val Acc: 80.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.5914, Train Acc: 70.00%, Val Loss: 0.7849, Val Acc: 70.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.5155, Train Acc: 75.00%, Val Loss: 0.7332, Val Acc: 70.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.6360, Train Acc: 67.50%, Val Loss: 0.9053, Val Acc: 70.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 0.6333, Train Acc: 65.00%, Val Loss: 0.8445, Val Acc: 40.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 0.7653, Train Acc: 60.00%, Val Loss: 0.8736, Val Acc: 60.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 0.6684, Train Acc: 65.00%, Val Loss: 0.8522, Val Acc: 50.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 0.7441, Train Acc: 70.00%, Val Loss: 0.7957, Val Acc: 60.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 0.5470, Train Acc: 77.50%, Val Loss: 0.8423, Val Acc: 60.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 0.6676, Train Acc: 67.50%, Val Loss: 0.7780, Val Acc: 80.00%\n",
            "Starting epoch 43\n",
            "Train Loss: 0.7169, Train Acc: 70.00%, Val Loss: 0.9616, Val Acc: 50.00%\n",
            "Starting epoch 44\n",
            "Train Loss: 0.5829, Train Acc: 70.00%, Val Loss: 1.0295, Val Acc: 60.00%\n",
            "Starting epoch 45\n",
            "Train Loss: 0.5094, Train Acc: 75.00%, Val Loss: 0.8618, Val Acc: 60.00%\n",
            "Starting epoch 46\n",
            "Train Loss: 0.5659, Train Acc: 80.00%, Val Loss: 0.9371, Val Acc: 50.00%\n",
            "Starting epoch 47\n",
            "Train Loss: 0.8250, Train Acc: 55.00%, Val Loss: 0.7504, Val Acc: 70.00%\n",
            "Starting epoch 48\n",
            "Train Loss: 0.6123, Train Acc: 72.50%, Val Loss: 0.7830, Val Acc: 60.00%\n",
            "Starting epoch 49\n",
            "Train Loss: 0.7397, Train Acc: 67.50%, Val Loss: 0.9766, Val Acc: 60.00%\n",
            "Starting epoch 50\n",
            "Train Loss: 0.6766, Train Acc: 70.00%, Val Loss: 1.0023, Val Acc: 50.00%\n",
            "--------------------------------\n",
            "Best validation accuracy of 0.00% achieved, model saved as c2_mobilenet_best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load('c2_mobilenet_best_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_atL46PB8qC",
        "outputId": "b648d476-e0c2-43ec-fc26-fd7ffd1c4888"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_indices = [i for i, (_, label) in enumerate(testset) if label in selected_classes]\n",
        "N = 1000\n",
        "class_counts = {label: 0 for label in selected_classes}\n",
        "filtered_train_indices = []\n",
        "\n",
        "for i in test_indices:\n",
        "    _, label = testset[i]\n",
        "    if class_counts[label] < N:\n",
        "        filtered_train_indices.append(i)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "test_subset = Subset(testset, filtered_train_indices)\n",
        "test_loader = DataLoader(test_subset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "e_TlF9stCEaa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Assuming the test_loader and the device are already defined\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Evaluation\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# No gradient is needed for evaluation\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        # Move data and targets to the correct device\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Compute the model output\n",
        "        output = model(data)\n",
        "        loss = criterion(output, targets)\n",
        "\n",
        "        # Accumulate the loss and calculate accuracy\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "# Calculate average loss and accuracy percentage\n",
        "modilenet_avg_loss = test_loss / len(test_loader)\n",
        "mobilenet_accuracy = 100. * correct / total\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f'Test Loss: {modilenet_avg_loss:.4f}, Test Accuracy: {mobilenet_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SkisKASCG_A",
        "outputId": "422591af-388f-414d-c450-c8c6eeb31247"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9879, Test Accuracy: 68.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet"
      ],
      "metadata": {
        "id": "vuaj9MjXCMXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "num_classes = 10\n",
        "dropout_rate = 0.5\n",
        "\n",
        "# Modify the fully connected layer\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(dropout_rate),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bifaFbVBCL3x",
        "outputId": "91eb912d-3b4d-4da1-c602-7bed682838f4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 172MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_accuracy = 0\n",
        "best_model_weights = None"
      ],
      "metadata": {
        "id": "WzouxHYYCW4D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(trainset)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    train_indices = [i for i, (_, label) in enumerate(trainset) if label in selected_classes]\n",
        "\n",
        "    N = 25\n",
        "    class_counts = {label: 0 for label in selected_classes}\n",
        "    filtered_train_indices = []\n",
        "\n",
        "    for i in train_indices:\n",
        "        _, label = trainset[i]\n",
        "        if class_counts[label] < N:\n",
        "            filtered_train_indices.append(i)\n",
        "            class_counts[label] += 1\n",
        "\n",
        "\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(filtered_train_indices)\n",
        "    split = int(0.8 * len(filtered_train_indices))\n",
        "    train_idx, val_idx = filtered_train_indices[:split], filtered_train_indices[split:]\n",
        "\n",
        "    train_subset = Subset(trainset, train_idx)\n",
        "    transformed_train_subset = TransformSubset(train_subset, transform=train_transform)\n",
        "\n",
        "    val_subset = Subset(trainset, val_idx)\n",
        "    transformed_val_subset = TransformSubset(val_subset, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(transformed_train_subset, batch_size=5, shuffle=True)\n",
        "    val_loader = DataLoader(transformed_val_subset, batch_size=5, shuffle=False)\n",
        "\n",
        "\n",
        "    # Init the neural network\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.005)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 25\n",
        "\n",
        "\n",
        "    # Run the training loop for defined number of epochs\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "        # Perform training and validation\n",
        "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, criterion)\n",
        "        val_loss, val_accuracy = validate(model, device, val_loader, criterion)\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Save the model if it has the best val accuracy so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_weights = model.state_dict().copy()  # Save the best model weights\n",
        "            patience_counter = 0  # Reset patience\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break  # Stop training if no improvement\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print('--------------------------------')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_weights, 'c2_Resnet_best_model.pth')\n",
        "print(f'Best validation accuracy of {best_val_accuracy:.2f}% achieved, model saved as c2_Resnet_best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceXR9bLeCb8_",
        "outputId": "ee7393c4-4705-4468-e1b6-8d979657805d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 2.6767, Train Acc: 50.00%, Val Loss: 27548.7178, Val Acc: 40.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 3.0435, Train Acc: 45.00%, Val Loss: 1532.3817, Val Acc: 40.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 2.1125, Train Acc: 50.00%, Val Loss: 219.8216, Val Acc: 60.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 2.0548, Train Acc: 52.50%, Val Loss: 457.2386, Val Acc: 50.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 1.9019, Train Acc: 55.00%, Val Loss: 120.7160, Val Acc: 50.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 2.1822, Train Acc: 57.50%, Val Loss: 27.3671, Val Acc: 40.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 1.4998, Train Acc: 47.50%, Val Loss: 132.2333, Val Acc: 50.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 1.4128, Train Acc: 47.50%, Val Loss: 10.2816, Val Acc: 50.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 1.5218, Train Acc: 47.50%, Val Loss: 2.0399, Val Acc: 50.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 1.4289, Train Acc: 52.50%, Val Loss: 3.6114, Val Acc: 50.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 1.1345, Train Acc: 52.50%, Val Loss: 0.7972, Val Acc: 60.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 1.0514, Train Acc: 52.50%, Val Loss: 0.8086, Val Acc: 40.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 1.1730, Train Acc: 50.00%, Val Loss: 0.7804, Val Acc: 40.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 1.4279, Train Acc: 50.00%, Val Loss: 0.9299, Val Acc: 40.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 2.8295, Train Acc: 42.50%, Val Loss: 0.6714, Val Acc: 70.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 1.0383, Train Acc: 50.00%, Val Loss: 16.2846, Val Acc: 60.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 1.2920, Train Acc: 47.50%, Val Loss: 31.5525, Val Acc: 50.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.8842, Train Acc: 70.00%, Val Loss: 6.3666, Val Acc: 50.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 1.4790, Train Acc: 45.00%, Val Loss: 2.4892, Val Acc: 50.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 1.2684, Train Acc: 55.00%, Val Loss: 1.3369, Val Acc: 40.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 1.1392, Train Acc: 47.50%, Val Loss: 0.6159, Val Acc: 50.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 1.4138, Train Acc: 47.50%, Val Loss: 0.5911, Val Acc: 50.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 1.2230, Train Acc: 45.00%, Val Loss: 0.6139, Val Acc: 50.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.8575, Train Acc: 50.00%, Val Loss: 0.6530, Val Acc: 50.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 1.4377, Train Acc: 55.00%, Val Loss: 0.7196, Val Acc: 50.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 1.1951, Train Acc: 60.00%, Val Loss: 0.6396, Val Acc: 50.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 1.0485, Train Acc: 47.50%, Val Loss: 0.6465, Val Acc: 50.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.9953, Train Acc: 50.00%, Val Loss: 0.6760, Val Acc: 50.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 1.0810, Train Acc: 60.00%, Val Loss: 0.7444, Val Acc: 40.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 1.2170, Train Acc: 57.50%, Val Loss: 1.7640, Val Acc: 50.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.7667, Train Acc: 60.00%, Val Loss: 0.6610, Val Acc: 50.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.9697, Train Acc: 67.50%, Val Loss: 0.6443, Val Acc: 50.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 1.6860, Train Acc: 57.50%, Val Loss: 0.6863, Val Acc: 50.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 1.3500, Train Acc: 45.00%, Val Loss: 0.6498, Val Acc: 50.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 1.2843, Train Acc: 45.00%, Val Loss: 0.6474, Val Acc: 50.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 1.4448, Train Acc: 42.50%, Val Loss: 0.7486, Val Acc: 40.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 0.9678, Train Acc: 47.50%, Val Loss: 1.0147, Val Acc: 40.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 1.3639, Train Acc: 42.50%, Val Loss: 1.3071, Val Acc: 50.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 1.4975, Train Acc: 52.50%, Val Loss: 1.0672, Val Acc: 40.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 1.3372, Train Acc: 37.50%, Val Loss: 0.7587, Val Acc: 40.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 1.2788, Train Acc: 25.00%, Val Loss: 0.6459, Val Acc: 50.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 0.7860, Train Acc: 60.00%, Val Loss: 0.6740, Val Acc: 50.00%\n",
            "Starting epoch 43\n",
            "Train Loss: 1.7042, Train Acc: 52.50%, Val Loss: 1.2363, Val Acc: 50.00%\n",
            "Starting epoch 44\n",
            "Train Loss: 0.8052, Train Acc: 57.50%, Val Loss: 1.2097, Val Acc: 50.00%\n",
            "Starting epoch 45\n",
            "Train Loss: 1.0486, Train Acc: 47.50%, Val Loss: 1.3828, Val Acc: 50.00%\n",
            "Starting epoch 46\n",
            "Train Loss: 1.0370, Train Acc: 55.00%, Val Loss: 1.5060, Val Acc: 50.00%\n",
            "Starting epoch 47\n",
            "Train Loss: 0.9156, Train Acc: 67.50%, Val Loss: 0.8611, Val Acc: 40.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 1.6575, Train Acc: 47.50%, Val Loss: 0.6839, Val Acc: 60.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 1.7488, Train Acc: 40.00%, Val Loss: 1.0563, Val Acc: 40.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 1.3968, Train Acc: 52.50%, Val Loss: 1.0060, Val Acc: 40.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 1.5188, Train Acc: 40.00%, Val Loss: 0.6311, Val Acc: 60.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.9976, Train Acc: 57.50%, Val Loss: 0.7583, Val Acc: 40.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 1.0888, Train Acc: 47.50%, Val Loss: 0.9796, Val Acc: 40.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 1.5224, Train Acc: 42.50%, Val Loss: 8.0381, Val Acc: 20.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 1.7280, Train Acc: 47.50%, Val Loss: 0.7108, Val Acc: 30.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 1.4785, Train Acc: 50.00%, Val Loss: 0.6644, Val Acc: 40.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.8992, Train Acc: 62.50%, Val Loss: 0.7106, Val Acc: 40.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 1.3925, Train Acc: 50.00%, Val Loss: 0.7245, Val Acc: 40.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.7849, Train Acc: 45.00%, Val Loss: 0.7060, Val Acc: 40.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 1.2641, Train Acc: 52.50%, Val Loss: 0.8321, Val Acc: 30.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 1.0085, Train Acc: 62.50%, Val Loss: 0.7565, Val Acc: 40.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 1.2022, Train Acc: 62.50%, Val Loss: 0.8799, Val Acc: 30.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 1.1848, Train Acc: 55.00%, Val Loss: 0.8940, Val Acc: 30.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 1.1598, Train Acc: 47.50%, Val Loss: 0.7884, Val Acc: 30.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 1.4113, Train Acc: 45.00%, Val Loss: 0.9553, Val Acc: 30.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 1.2993, Train Acc: 42.50%, Val Loss: 0.8175, Val Acc: 30.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.9235, Train Acc: 50.00%, Val Loss: 0.7399, Val Acc: 40.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.7863, Train Acc: 70.00%, Val Loss: 0.7095, Val Acc: 40.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 1.1240, Train Acc: 50.00%, Val Loss: 0.6926, Val Acc: 40.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.7377, Train Acc: 57.50%, Val Loss: 0.6925, Val Acc: 40.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.9721, Train Acc: 55.00%, Val Loss: 0.7627, Val Acc: 30.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.9250, Train Acc: 45.00%, Val Loss: 0.7356, Val Acc: 40.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 1.9048, Train Acc: 52.50%, Val Loss: 0.7135, Val Acc: 40.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 1.5455, Train Acc: 32.50%, Val Loss: 0.7263, Val Acc: 40.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.6822, Train Acc: 62.50%, Val Loss: 0.7290, Val Acc: 40.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 1.0946, Train Acc: 50.00%, Val Loss: 0.7854, Val Acc: 30.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 2.3768, Train Acc: 55.00%, Val Loss: 1.1766, Val Acc: 40.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 1.9271, Train Acc: 57.50%, Val Loss: 13.5921, Val Acc: 40.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 2.1946, Train Acc: 50.00%, Val Loss: 34.3752, Val Acc: 60.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 1.0152, Train Acc: 60.00%, Val Loss: 0.8706, Val Acc: 60.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 1.0954, Train Acc: 40.00%, Val Loss: 0.7497, Val Acc: 50.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 1.3076, Train Acc: 50.00%, Val Loss: 0.9207, Val Acc: 50.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 1.6080, Train Acc: 55.00%, Val Loss: 0.6820, Val Acc: 60.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 1.1194, Train Acc: 60.00%, Val Loss: 0.7269, Val Acc: 60.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 1.1178, Train Acc: 40.00%, Val Loss: 1.4730, Val Acc: 50.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 1.0677, Train Acc: 50.00%, Val Loss: 1.2658, Val Acc: 40.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 1.3377, Train Acc: 45.00%, Val Loss: 0.8585, Val Acc: 30.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 1.1301, Train Acc: 52.50%, Val Loss: 0.7866, Val Acc: 40.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.8387, Train Acc: 52.50%, Val Loss: 0.8523, Val Acc: 60.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.8804, Train Acc: 65.00%, Val Loss: 0.8766, Val Acc: 60.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.5458, Train Acc: 65.00%, Val Loss: 0.8825, Val Acc: 60.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.9657, Train Acc: 60.00%, Val Loss: 0.9029, Val Acc: 60.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.8944, Train Acc: 52.50%, Val Loss: 0.8817, Val Acc: 70.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.8414, Train Acc: 62.50%, Val Loss: 0.9433, Val Acc: 70.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.8484, Train Acc: 60.00%, Val Loss: 0.9134, Val Acc: 60.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.6420, Train Acc: 62.50%, Val Loss: 0.8672, Val Acc: 70.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.8105, Train Acc: 60.00%, Val Loss: 0.8954, Val Acc: 70.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 1.3402, Train Acc: 52.50%, Val Loss: 0.8862, Val Acc: 70.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.8070, Train Acc: 60.00%, Val Loss: 0.9777, Val Acc: 60.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.9118, Train Acc: 50.00%, Val Loss: 0.9836, Val Acc: 60.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.8874, Train Acc: 47.50%, Val Loss: 0.9561, Val Acc: 70.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.8984, Train Acc: 50.00%, Val Loss: 0.9304, Val Acc: 70.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.9985, Train Acc: 55.00%, Val Loss: 0.9223, Val Acc: 60.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.7429, Train Acc: 67.50%, Val Loss: 0.8984, Val Acc: 60.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.6120, Train Acc: 65.00%, Val Loss: 0.9363, Val Acc: 60.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.7899, Train Acc: 62.50%, Val Loss: 0.8985, Val Acc: 60.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 1.3273, Train Acc: 42.50%, Val Loss: 0.9128, Val Acc: 60.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.8932, Train Acc: 52.50%, Val Loss: 0.9429, Val Acc: 60.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 2.1156, Train Acc: 52.50%, Val Loss: 0.8950, Val Acc: 40.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 1.8837, Train Acc: 42.50%, Val Loss: 10.1386, Val Acc: 40.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 1.4154, Train Acc: 42.50%, Val Loss: 1.6655, Val Acc: 40.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 1.0259, Train Acc: 47.50%, Val Loss: 5.4155, Val Acc: 40.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 1.2573, Train Acc: 42.50%, Val Loss: 1.9654, Val Acc: 30.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.8478, Train Acc: 55.00%, Val Loss: 0.6725, Val Acc: 60.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.6536, Train Acc: 70.00%, Val Loss: 0.5816, Val Acc: 70.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.9004, Train Acc: 57.50%, Val Loss: 0.6433, Val Acc: 70.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.9848, Train Acc: 45.00%, Val Loss: 0.6778, Val Acc: 50.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 1.0019, Train Acc: 50.00%, Val Loss: 0.6448, Val Acc: 60.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.9000, Train Acc: 55.00%, Val Loss: 0.5904, Val Acc: 70.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.6965, Train Acc: 65.00%, Val Loss: 0.5750, Val Acc: 60.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.8309, Train Acc: 60.00%, Val Loss: 0.5987, Val Acc: 60.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.8154, Train Acc: 57.50%, Val Loss: 0.5677, Val Acc: 80.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.9973, Train Acc: 55.00%, Val Loss: 0.5799, Val Acc: 80.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.8647, Train Acc: 55.00%, Val Loss: 0.5527, Val Acc: 80.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.7999, Train Acc: 65.00%, Val Loss: 0.5745, Val Acc: 70.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 1.0956, Train Acc: 42.50%, Val Loss: 0.5794, Val Acc: 70.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.5357, Train Acc: 75.00%, Val Loss: 0.5795, Val Acc: 70.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.8011, Train Acc: 55.00%, Val Loss: 0.5702, Val Acc: 70.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.6198, Train Acc: 77.50%, Val Loss: 0.5829, Val Acc: 70.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.6367, Train Acc: 70.00%, Val Loss: 0.5613, Val Acc: 70.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.6954, Train Acc: 67.50%, Val Loss: 0.5871, Val Acc: 70.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.6045, Train Acc: 62.50%, Val Loss: 0.5963, Val Acc: 70.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.9070, Train Acc: 60.00%, Val Loss: 0.5800, Val Acc: 70.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.9268, Train Acc: 62.50%, Val Loss: 0.5840, Val Acc: 70.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.8887, Train Acc: 62.50%, Val Loss: 0.5634, Val Acc: 70.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.5011, Train Acc: 75.00%, Val Loss: 0.5649, Val Acc: 70.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.6862, Train Acc: 75.00%, Val Loss: 0.5753, Val Acc: 70.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.7974, Train Acc: 60.00%, Val Loss: 0.5797, Val Acc: 70.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.6246, Train Acc: 72.50%, Val Loss: 0.5745, Val Acc: 70.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.8021, Train Acc: 70.00%, Val Loss: 0.5946, Val Acc: 70.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.6014, Train Acc: 77.50%, Val Loss: 0.5771, Val Acc: 70.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.8222, Train Acc: 65.00%, Val Loss: 0.5642, Val Acc: 70.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.5842, Train Acc: 70.00%, Val Loss: 0.5987, Val Acc: 70.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.7180, Train Acc: 77.50%, Val Loss: 0.5844, Val Acc: 70.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 0.7047, Train Acc: 62.50%, Val Loss: 0.5757, Val Acc: 70.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 0.7673, Train Acc: 70.00%, Val Loss: 0.5467, Val Acc: 70.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 0.9175, Train Acc: 60.00%, Val Loss: 0.5306, Val Acc: 70.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 0.8435, Train Acc: 52.50%, Val Loss: 0.5520, Val Acc: 70.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 0.8766, Train Acc: 52.50%, Val Loss: 0.5910, Val Acc: 70.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 0.8198, Train Acc: 62.50%, Val Loss: 0.5886, Val Acc: 70.00%\n",
            "Starting epoch 43\n",
            "Train Loss: 0.5673, Train Acc: 72.50%, Val Loss: 0.5902, Val Acc: 70.00%\n",
            "Starting epoch 44\n",
            "Train Loss: 1.0287, Train Acc: 57.50%, Val Loss: 0.5668, Val Acc: 70.00%\n",
            "Starting epoch 45\n",
            "Train Loss: 0.9323, Train Acc: 55.00%, Val Loss: 0.5426, Val Acc: 70.00%\n",
            "Starting epoch 46\n",
            "Train Loss: 0.6049, Train Acc: 75.00%, Val Loss: 0.5315, Val Acc: 80.00%\n",
            "Starting epoch 47\n",
            "Train Loss: 0.6692, Train Acc: 60.00%, Val Loss: 0.5703, Val Acc: 70.00%\n",
            "Starting epoch 48\n",
            "Train Loss: 0.8683, Train Acc: 60.00%, Val Loss: 0.5905, Val Acc: 70.00%\n",
            "Starting epoch 49\n",
            "Train Loss: 1.1318, Train Acc: 50.00%, Val Loss: 0.5874, Val Acc: 70.00%\n",
            "Starting epoch 50\n",
            "Train Loss: 0.7363, Train Acc: 60.00%, Val Loss: 0.5787, Val Acc: 70.00%\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 1.2921, Train Acc: 55.00%, Val Loss: 0.9202, Val Acc: 60.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 2.0557, Train Acc: 42.50%, Val Loss: 13.9871, Val Acc: 40.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 1.1681, Train Acc: 42.50%, Val Loss: 0.7847, Val Acc: 70.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 1.0919, Train Acc: 55.00%, Val Loss: 32.0109, Val Acc: 40.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.8694, Train Acc: 47.50%, Val Loss: 0.8347, Val Acc: 60.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 1.0484, Train Acc: 47.50%, Val Loss: 0.7564, Val Acc: 50.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.7683, Train Acc: 65.00%, Val Loss: 0.8556, Val Acc: 40.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 1.0011, Train Acc: 52.50%, Val Loss: 0.8192, Val Acc: 60.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 1.0540, Train Acc: 55.00%, Val Loss: 0.9839, Val Acc: 50.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 1.0995, Train Acc: 45.00%, Val Loss: 0.9070, Val Acc: 40.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 1.0005, Train Acc: 45.00%, Val Loss: 0.6545, Val Acc: 70.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.9866, Train Acc: 52.50%, Val Loss: 0.5987, Val Acc: 70.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.8326, Train Acc: 67.50%, Val Loss: 0.5651, Val Acc: 70.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.6252, Train Acc: 72.50%, Val Loss: 0.6007, Val Acc: 60.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.7818, Train Acc: 62.50%, Val Loss: 0.6703, Val Acc: 60.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.7661, Train Acc: 67.50%, Val Loss: 0.7235, Val Acc: 60.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.6876, Train Acc: 50.00%, Val Loss: 0.7183, Val Acc: 60.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 1.0210, Train Acc: 47.50%, Val Loss: 0.7176, Val Acc: 60.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.9604, Train Acc: 52.50%, Val Loss: 0.6976, Val Acc: 60.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.6397, Train Acc: 62.50%, Val Loss: 0.7144, Val Acc: 50.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.7315, Train Acc: 52.50%, Val Loss: 0.7267, Val Acc: 40.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 1.0770, Train Acc: 55.00%, Val Loss: 0.7263, Val Acc: 40.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.7330, Train Acc: 62.50%, Val Loss: 0.7067, Val Acc: 60.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.6859, Train Acc: 65.00%, Val Loss: 0.7104, Val Acc: 40.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.9140, Train Acc: 60.00%, Val Loss: 0.7036, Val Acc: 40.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.8558, Train Acc: 52.50%, Val Loss: 0.6797, Val Acc: 50.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.9204, Train Acc: 47.50%, Val Loss: 0.6779, Val Acc: 40.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.8195, Train Acc: 47.50%, Val Loss: 0.6957, Val Acc: 60.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.7240, Train Acc: 65.00%, Val Loss: 0.6940, Val Acc: 60.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.7689, Train Acc: 60.00%, Val Loss: 0.6925, Val Acc: 60.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.7044, Train Acc: 65.00%, Val Loss: 0.7027, Val Acc: 50.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.7311, Train Acc: 62.50%, Val Loss: 0.7153, Val Acc: 40.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.7718, Train Acc: 57.50%, Val Loss: 0.6886, Val Acc: 40.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.8423, Train Acc: 60.00%, Val Loss: 0.7131, Val Acc: 40.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.8281, Train Acc: 52.50%, Val Loss: 0.6771, Val Acc: 40.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.6918, Train Acc: 57.50%, Val Loss: 0.6973, Val Acc: 50.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 1.0583, Train Acc: 52.50%, Val Loss: 0.7166, Val Acc: 40.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 0.9469, Train Acc: 45.00%, Val Loss: 0.7245, Val Acc: 50.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "Best validation accuracy of 0.00% achieved, model saved as c2_Resnet_best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load('c2_Resnet_best_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRxcvY0rCdjS",
        "outputId": "d9587d06-fbd9-4d60-9e42-e266bc6c95cf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_indices = [i for i, (_, label) in enumerate(testset) if label in selected_classes]\n",
        "N = 1000\n",
        "class_counts = {label: 0 for label in selected_classes}\n",
        "filtered_train_indices = []\n",
        "\n",
        "for i in test_indices:\n",
        "    _, label = testset[i]\n",
        "    if class_counts[label] < N:\n",
        "        filtered_train_indices.append(i)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "test_subset = Subset(testset, filtered_train_indices)\n",
        "test_loader = DataLoader(test_subset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "WYbIK5kRCoZn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Assuming the test_loader and the device are already defined\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Evaluation\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# No gradient is needed for evaluation\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        # Move data and targets to the correct device\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Compute the model output\n",
        "        output = model(data)\n",
        "        loss = criterion(output, targets)\n",
        "\n",
        "        # Accumulate the loss and calculate accuracy\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "# Calculate average loss and accuracy percentage\n",
        "Resnet_avg_loss = test_loss / len(test_loader)\n",
        "Resnet_accuracy = 100. * correct / total\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f'Test Loss: {Resnet_avg_loss:.4f}, Test Accuracy: {Resnet_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZWAKCVRCqtf",
        "outputId": "b430d422-5b4c-4664-bcb4-879fe3502c33"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6758, Test Accuracy: 63.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIT"
      ],
      "metadata": {
        "id": "CN5JTWFlC3y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "j7LW-y62DX01"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "])"
      ],
      "metadata": {
        "id": "hIEWhjTDDcJZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vit_b_16(pretrained=True)\n",
        "\n",
        "num_classes = 10\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model.heads = nn.Sequential(\n",
        "    nn.Dropout(p=dropout_rate),\n",
        "    nn.Linear(model.heads[0].in_features, num_classes)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iujstwiYDfKC",
        "outputId": "25bf555a-c670-4cca-e958-42b285fc90d9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:03<00:00, 86.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "3HcjzQ4THdS5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, device, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, targets) in enumerate(val_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "4tIG9uE9HeRo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_accuracy = 0\n",
        "best_model_weights = None"
      ],
      "metadata": {
        "id": "_ujjy9RbDif1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(trainset)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    train_indices = [i for i, (_, label) in enumerate(trainset) if label in selected_classes]\n",
        "\n",
        "    N = 25\n",
        "    class_counts = {label: 0 for label in selected_classes}\n",
        "    filtered_train_indices = []\n",
        "\n",
        "    for i in train_indices:\n",
        "        _, label = trainset[i]\n",
        "        if class_counts[label] < N:\n",
        "            filtered_train_indices.append(i)\n",
        "            class_counts[label] += 1\n",
        "\n",
        "\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(filtered_train_indices)\n",
        "    split = int(0.8 * len(filtered_train_indices))\n",
        "    train_idx, val_idx = filtered_train_indices[:split], filtered_train_indices[split:]\n",
        "\n",
        "    train_subset = Subset(trainset, train_idx)\n",
        "    transformed_train_subset = TransformSubset(train_subset, transform=train_transform)\n",
        "\n",
        "    val_subset = Subset(trainset, val_idx)\n",
        "    transformed_val_subset = TransformSubset(val_subset, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(transformed_train_subset, batch_size=5, shuffle=True)\n",
        "    val_loader = DataLoader(transformed_val_subset, batch_size=5, shuffle=False)\n",
        "\n",
        "\n",
        "    # Init the neural network\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 25\n",
        "\n",
        "\n",
        "    # Run the training loop for defined number of epochs\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "        # Perform training and validation\n",
        "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, criterion)\n",
        "        val_loss, val_accuracy = validate(model, device, val_loader, criterion)\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Save the model if it has the best val accuracy so far\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_weights = model.state_dict().copy()  # Save the best model weights\n",
        "            patience_counter = 0  # Reset patience\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break  # Stop training if no improvement\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    print('--------------------------------')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_weights, 'c2_vit_best_model.pth')\n",
        "print(f'Best validation accuracy of {best_val_accuracy:.2f}% achieved, model saved as c2_vit_best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrQqAqLSDlNA",
        "outputId": "31c4d1be-9062-42c9-c952-80e582dda364"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 1.6557, Train Acc: 45.00%, Val Loss: 0.6546, Val Acc: 100.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.4343, Train Acc: 95.00%, Val Loss: 0.2905, Val Acc: 100.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 0.1765, Train Acc: 100.00%, Val Loss: 0.1595, Val Acc: 100.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.1135, Train Acc: 100.00%, Val Loss: 0.0880, Val Acc: 100.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.0756, Train Acc: 100.00%, Val Loss: 0.0503, Val Acc: 100.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.0230, Train Acc: 100.00%, Val Loss: 0.0366, Val Acc: 100.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.0188, Train Acc: 100.00%, Val Loss: 0.0290, Val Acc: 100.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.0170, Train Acc: 100.00%, Val Loss: 0.0235, Val Acc: 100.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.0135, Train Acc: 100.00%, Val Loss: 0.0195, Val Acc: 100.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.0110, Train Acc: 100.00%, Val Loss: 0.0162, Val Acc: 100.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.0108, Train Acc: 100.00%, Val Loss: 0.0149, Val Acc: 100.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.0077, Train Acc: 100.00%, Val Loss: 0.0141, Val Acc: 100.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.0077, Train Acc: 100.00%, Val Loss: 0.0134, Val Acc: 100.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.0085, Train Acc: 100.00%, Val Loss: 0.0128, Val Acc: 100.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.0073, Train Acc: 100.00%, Val Loss: 0.0121, Val Acc: 100.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.0070, Train Acc: 100.00%, Val Loss: 0.0115, Val Acc: 100.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.0064, Train Acc: 100.00%, Val Loss: 0.0111, Val Acc: 100.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.0062, Train Acc: 100.00%, Val Loss: 0.0106, Val Acc: 100.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.0059, Train Acc: 100.00%, Val Loss: 0.0101, Val Acc: 100.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.0060, Train Acc: 100.00%, Val Loss: 0.0097, Val Acc: 100.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.0054, Train Acc: 100.00%, Val Loss: 0.0095, Val Acc: 100.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.0057, Train Acc: 100.00%, Val Loss: 0.0093, Val Acc: 100.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.0060, Train Acc: 100.00%, Val Loss: 0.0091, Val Acc: 100.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.0052, Train Acc: 100.00%, Val Loss: 0.0088, Val Acc: 100.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.0048, Train Acc: 100.00%, Val Loss: 0.0085, Val Acc: 100.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.0057, Train Acc: 100.00%, Val Loss: 0.0083, Val Acc: 100.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.0042, Train Acc: 100.00%, Val Loss: 0.0081, Val Acc: 100.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.0040, Train Acc: 100.00%, Val Loss: 0.0080, Val Acc: 100.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.0050, Train Acc: 100.00%, Val Loss: 0.0078, Val Acc: 100.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.0048, Train Acc: 100.00%, Val Loss: 0.0076, Val Acc: 100.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.0045, Train Acc: 100.00%, Val Loss: 0.0076, Val Acc: 100.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.0037, Train Acc: 100.00%, Val Loss: 0.0075, Val Acc: 100.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.0038, Train Acc: 100.00%, Val Loss: 0.0074, Val Acc: 100.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.0043, Train Acc: 100.00%, Val Loss: 0.0074, Val Acc: 100.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.0038, Train Acc: 100.00%, Val Loss: 0.0073, Val Acc: 100.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.0051, Train Acc: 100.00%, Val Loss: 0.0072, Val Acc: 100.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 0.0043, Train Acc: 100.00%, Val Loss: 0.0072, Val Acc: 100.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 0.0041, Train Acc: 100.00%, Val Loss: 0.0071, Val Acc: 100.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 0.0036, Train Acc: 100.00%, Val Loss: 0.0070, Val Acc: 100.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 0.0036, Train Acc: 100.00%, Val Loss: 0.0070, Val Acc: 100.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 0.0037, Train Acc: 100.00%, Val Loss: 0.0069, Val Acc: 100.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 0.0038, Train Acc: 100.00%, Val Loss: 0.0069, Val Acc: 100.00%\n",
            "Starting epoch 43\n",
            "Train Loss: 0.0032, Train Acc: 100.00%, Val Loss: 0.0068, Val Acc: 100.00%\n",
            "Starting epoch 44\n",
            "Train Loss: 0.0036, Train Acc: 100.00%, Val Loss: 0.0068, Val Acc: 100.00%\n",
            "Starting epoch 45\n",
            "Train Loss: 0.0037, Train Acc: 100.00%, Val Loss: 0.0068, Val Acc: 100.00%\n",
            "Starting epoch 46\n",
            "Train Loss: 0.0040, Train Acc: 100.00%, Val Loss: 0.0067, Val Acc: 100.00%\n",
            "Starting epoch 47\n",
            "Train Loss: 0.0039, Train Acc: 100.00%, Val Loss: 0.0067, Val Acc: 100.00%\n",
            "Starting epoch 48\n",
            "Train Loss: 0.0031, Train Acc: 100.00%, Val Loss: 0.0067, Val Acc: 100.00%\n",
            "Starting epoch 49\n",
            "Train Loss: 0.0041, Train Acc: 100.00%, Val Loss: 0.0066, Val Acc: 100.00%\n",
            "Starting epoch 50\n",
            "Train Loss: 0.0029, Train Acc: 100.00%, Val Loss: 0.0066, Val Acc: 100.00%\n",
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 0.0024, Train Acc: 100.00%, Val Loss: 0.0019, Val Acc: 100.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.0010, Train Acc: 100.00%, Val Loss: 0.0010, Val Acc: 100.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.0005, Val Acc: 100.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.0003, Train Acc: 100.00%, Val Loss: 0.0003, Val Acc: 100.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 43\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 44\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 45\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 46\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 47\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 48\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 49\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 50\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0003, Val Acc: 100.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 31\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 32\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 33\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 34\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 35\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 36\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 37\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 38\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 39\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 40\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 41\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 42\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 43\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 44\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 45\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 46\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 47\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 48\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 49\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 50\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 1.1989, Val Acc: 90.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.0869, Train Acc: 95.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.0002, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.0024, Train Acc: 100.00%, Val Loss: 0.0007, Val Acc: 100.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.0010, Train Acc: 100.00%, Val Loss: 0.0007, Val Acc: 100.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.0009, Train Acc: 100.00%, Val Loss: 0.0007, Val Acc: 100.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.0032, Train Acc: 100.00%, Val Loss: 0.0006, Val Acc: 100.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.0007, Train Acc: 100.00%, Val Loss: 0.0006, Val Acc: 100.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.0006, Val Acc: 100.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.0013, Train Acc: 100.00%, Val Loss: 0.0006, Val Acc: 100.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0005, Val Acc: 100.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.0003, Train Acc: 100.00%, Val Loss: 0.0005, Val Acc: 100.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0005, Val Acc: 100.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0005, Val Acc: 100.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.0005, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.0003, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.0003, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.0003, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.0004, Train Acc: 100.00%, Val Loss: 0.0004, Val Acc: 100.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "Starting epoch 1\n",
            "Train Loss: 0.0001, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 2\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0003, Val Acc: 100.00%\n",
            "Starting epoch 3\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 4\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 5\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 6\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 7\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 8\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 9\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 10\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 11\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 12\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 13\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 14\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 15\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 16\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 17\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 18\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 19\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 20\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 21\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 22\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 23\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Starting epoch 24\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 25\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 26\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Starting epoch 27\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0002, Val Acc: 100.00%\n",
            "Starting epoch 28\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0003, Val Acc: 100.00%\n",
            "Starting epoch 29\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0003, Val Acc: 100.00%\n",
            "Starting epoch 30\n",
            "Train Loss: 0.0000, Train Acc: 100.00%, Val Loss: 0.0003, Val Acc: 100.00%\n",
            "Early stopping triggered.\n",
            "--------------------------------\n",
            "Best validation accuracy of 0.00% achieved, model saved as c2_vit_best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load('c2_vit_best_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd6ji6rvDrUc",
        "outputId": "44bdfee1-62a3-4731-9c98-28929959cf38"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twO97Q8xDy_X",
        "outputId": "e2ba4f85-b863-4c57-d1e3-ce2d5c3f5d3a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_indices = [i for i, (_, label) in enumerate(testset) if label in selected_classes]\n",
        "N = 1000\n",
        "class_counts = {label: 0 for label in selected_classes}\n",
        "filtered_train_indices = []\n",
        "\n",
        "for i in test_indices:\n",
        "    _, label = testset[i]\n",
        "    if class_counts[label] < N:\n",
        "        filtered_train_indices.append(i)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "test_subset = Subset(testset, filtered_train_indices)\n",
        "test_loader = DataLoader(test_subset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "5iKHld7MDvfr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Assuming the test_loader and the device are already defined\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Evaluation\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# No gradient is needed for evaluation\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        # Move data and targets to the correct device\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Compute the model output\n",
        "        output = model(data)\n",
        "        loss = criterion(output, targets)\n",
        "\n",
        "        # Accumulate the loss and calculate accuracy\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "# Calculate average loss and accuracy percentage\n",
        "vit_avg_loss = test_loss / len(test_loader)\n",
        "vit_accuracy = 100. * correct / total\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f'Test Loss: {vit_avg_loss:.4f}, Test Accuracy: {vit_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcLY0M3TD3b9",
        "outputId": "779d86d8-86b9-4666-cef7-4431a8645882"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0631, Test Accuracy: 99.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_classes_mapping = {0: \"Cat\", 1: \"Dog\", 2: \"Bird\", 3: \"Horse\", 4: \"Ship\", 5: \"Truck\", 6: \"Frog\", 7: \"Airplane\", 8: \"Deer\", 9: \"Automobile\"}\n",
        "selected_classes = [selected_classes_mapping[class_num] for class_num in selected_classes]\n",
        "print(f'Randomly selected from CIFAR-10 are {selected_classes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj1jMk_mEO6s",
        "outputId": "d8db9cfc-85da-4657-f974-54054378f498"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly selected from CIFAR-10 are ['Bird', 'Automobile']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Z6FkN4LeETJs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "bars = plt.bar(['MobileNet', 'ResNet', 'Vision Transformer'], [mobilenet_accuracy, Resnet_accuracy, vit_accuracy], color=['blue', 'orange', 'green'])\n",
        "plt.title('Accuracy Comparison')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.ylim(0, 110)\n",
        "\n",
        "# Annotate bars with values\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')\n",
        "\n",
        "# Loss comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "bars = plt.bar(['MobileNet', 'ResNet', 'Vision Transformer'], [modilenet_avg_loss, Resnet_avg_loss, vit_avg_loss], color=['blue', 'orange', 'green'])\n",
        "plt.title('Average Loss Comparison')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.ylim(0, 3.2)\n",
        "\n",
        "# Annotate bars with values\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 2), va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "EK-B161wEVgN",
        "outputId": "65ce6067-d3bb-429a-cd49-92f2a1cc5329"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}