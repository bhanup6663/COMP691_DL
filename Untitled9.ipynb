{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKC0puNPie7vasTzsmf+49",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanup6663/COMP691_DL/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSddn-X8_zXE",
        "outputId": "bf5c4a88-10c1-426c-e5ac-2a4608878c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:11<00:00, 14.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Evaluating ResNet-20 (Baseline)...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Define BasicBlock for ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define ResNet Architecture\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = nn.functional.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# ResNet model variants\n",
        "def ResNet20(num_classes=100):\n",
        "    return ResNet(BasicBlock, [3, 3, 3], num_classes)\n",
        "\n",
        "def ResNet32(num_classes=100):\n",
        "    return ResNet(BasicBlock, [5, 5, 5], num_classes)\n",
        "\n",
        "def ResNet44(num_classes=100):\n",
        "    return ResNet(BasicBlock, [7, 7, 7], num_classes)\n",
        "\n",
        "def ResNet56(num_classes=100):\n",
        "    return ResNet(BasicBlock, [9, 9, 9], num_classes)\n",
        "\n",
        "def ResNet110(num_classes=100):\n",
        "    return ResNet(BasicBlock, [18, 18, 18], num_classes)\n",
        "\n",
        "# Random Erasing for Data Augmentation\n",
        "class RandomErasing:\n",
        "    def __init__(self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
        "        self.probability = probability\n",
        "        self.sl = sl\n",
        "        self.sh = sh\n",
        "        self.r1 = r1\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.uniform(0, 1) > self.probability:\n",
        "            return img\n",
        "        for _ in range(100):\n",
        "            area = img.size()[1] * img.size()[2]\n",
        "            target_area = random.uniform(self.sl, self.sh) * area\n",
        "            aspect_ratio = random.uniform(self.r1, 1 / self.r1)\n",
        "            h = int(round(np.sqrt(target_area * aspect_ratio)))\n",
        "            w = int(round(np.sqrt(target_area / aspect_ratio)))\n",
        "            if w < img.size()[2] and h < img.size()[1]:\n",
        "                x1 = random.randint(0, img.size()[1] - h)\n",
        "                y1 = random.randint(0, img.size()[2] - w)\n",
        "                img[:, x1:x1 + h, y1:y1 + w] = torch.tensor(self.mean).view(-1, 1, 1)\n",
        "                return img\n",
        "        return img\n",
        "\n",
        "# Data Augmentation and Loading\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    RandomErasing(probability=0.5, mean=[0.4914, 0.4822, 0.4465])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Training Function\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "    return total_loss / len(loader), total_correct / len(loader.dataset)\n",
        "\n",
        "# Testing Function\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "    return total_loss / len(loader), total_correct / len(loader.dataset)\n",
        "\n",
        "# Evaluate Multiple Runs\n",
        "def evaluate_multiple_runs(model, train_loader, test_loader, criterion, optimizer, device, num_runs=5, epochs=100):\n",
        "    error_rates = []\n",
        "    for _ in range(num_runs):\n",
        "        model.apply(reset_weights)  # Reset weights for each run\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "        error_rates.append(1 - test_acc)  # Calculate error rate\n",
        "    return np.mean(error_rates), np.std(error_rates)\n",
        "\n",
        "def reset_weights(module):\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "        module.reset_parameters()\n",
        "\n",
        "# Main Evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "models_to_test = {\n",
        "    \"ResNet-20\": ResNet20(),\n",
        "    \"ResNet-32\": ResNet32(),\n",
        "    \"ResNet-44\": ResNet44(),\n",
        "    \"ResNet-56\": ResNet56(),\n",
        "    \"ResNet-110\": ResNet110(),\n",
        "}\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models_to_test.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    mean_baseline, std_baseline = evaluate_multiple_runs(model, train_loader, test_loader, criterion, optimizer, device, num_runs=5, epochs=100)\n",
        "    results[model_name] = {\"Baseline Error Rate\": mean_baseline, \"Baseline Std\": std_baseline}\n",
        "\n",
        "    print(f\"{model_name} Baseline Error Rate: {mean_baseline:.4f} ± {std_baseline:.4f}\")\n",
        "\n",
        "# Print Results\n",
        "print(\"\\nResults Summary:\")\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}: Baseline Error Rate: {metrics['Baseline Error Rate']:.4f}, Std: {metrics['Baseline Std']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, model in models_to_test.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    # Evaluate Baseline\n",
        "    mean_baseline, std_baseline = evaluate_multiple_runs(model, train_loader, test_loader, criterion, optimizer, device, num_runs=5, epochs=100)\n",
        "    results[model_name] = {\n",
        "        \"Baseline Error Rate\": mean_baseline,\n",
        "        \"Baseline Std\": std_baseline\n",
        "    }\n",
        "    print(f\"{model_name} Baseline Error Rate: {mean_baseline:.4f} ± {std_baseline:.4f}\")\n",
        "\n",
        "    # Evaluate with Random Erasing\n",
        "    print(f\"Evaluating {model_name} with Random Erasing...\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Reinitialize DataLoader\n",
        "    mean_erasing, std_erasing = evaluate_multiple_runs(model, train_loader, test_loader, criterion, optimizer, device, num_runs=5, epochs=100)\n",
        "    results[model_name][\"Random Erasing Error Rate\"] = mean_erasing\n",
        "    results[model_name][\"Random Erasing Std\"] = std_erasing\n",
        "    print(f\"{model_name} Random Erasing Error Rate: {mean_erasing:.4f} ± {std_erasing:.4f}\")\n"
      ],
      "metadata": {
        "id": "90cV6BagIvqc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}